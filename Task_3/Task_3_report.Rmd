---
title: "Project for Assignment 3"
output: pdf_document
date: "2024-11-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

Creating the Scatter Plot:
The black dots are considered Diabetes level 0 while red dots are considered Diabetes level 1.

1. Do you think that Diabetes is easy to classify by a standard logistic regression model that uses these two variables as features? 

We do not believe that Diabetes is easy to classify by a standard logistic regression model that uses only two variables (Age and Glucose Concentration) as features. If we look at the plot ![Plot of Age VS Plasma Glucose Concentration]("C:\Users\Ana\Documents\LiU University\Machine Learning\Lab solutions\ML_lab1\plots for labs\plot age vs plasma glucose concentration.png") we can notice that having high Glucose concentration level could be connected with having Diabetes (the red dots), while we cannot see such significant impact with Age as a factor. Meaning that perhaps a different factor compared to Age should be considered. Also, there seems to be quite a lot of overlap between the red and black points, hence, logistic regression might not be the best way of classifying as the decision boundary would not be linear.


2. Comment on the quality of the classification by using these results. 

![Plot of Age VS Plasma Glucose Concentration with predictions]("C:\Users\Ana\Documents\LiU University\Machine Learning\Lab solutions\ML_lab1\plots for labs\predicted diabetes = age vs glucose concentr.png")

Training Misclassification Error: 0.261194 

The model seems to give good predictions for higher glucose levels.There is visible overlap where class overlap in 110-150 region of Glucose Concentration which can lead to missclassification.This is expected, as logistic regression applies a linear decision boundary, which cannot capture more complex relationship in this region. We also see that classification boundary does not differentiate based on age, because the predictions for younger and older individuals in similar glucose ranges look similar, so Age as a factor has limited influence.

Probabilistic equation: $$P(y = 1 | x1, x2) = 1 / (1 + exp(-(intercept + coef_{x1} * x_1 + coef_{x2} * x_2)))$$


3. To derive the decision boundary we use the formula for probabilistic eq. and then we equate it to 0.5 and that is the treshold for which we devide the probabilities. So when we solve that equation for $x_2$ we get: $$x_2 = - \frac{intercept}{coef_{x2}} - \frac{coef_{x1}}{coef_{x2}} * x_1$$

![1. Plot of Age VS Plasma Glucose Concentration with decision boundary]("C:\Users\Ana\Documents\LiU University\Machine Learning\Lab solutions\ML_lab1\plots for labs\ predicted values plot with good decision boundary.png") 
Comment whether the decision boundary seems to catch the data distribution well?
The decision boundary cannot perfectly separate the two classes where they overlap,but it does separate most of the data appropriately. The slight miscllassification is because of the linear nature of logistic regression, while actual relationship between diabetes, Plasma glucose, and Age is probably not perfectly linear so logistic regression cannot capture it.


4. By using these plots, comment on what happens with the prediction when r value changes?

![1. Plot of Age VS Plasma Glucose Concentration using r = 0.2]("C:\Users\Ana\Documents\LiU University\Machine Learning\Lab solutions\ML_lab1\plots for labs\ age vs glucose with r=0.2.png")

![1. Plot of Age VS Plasma Glucose Concentration using r = 0.8]("C:\Users\Ana\Documents\LiU University\Machine Learning\Lab solutions\ML_lab1\plots for labs\ age vs glucose with r=0.8.png")

When the r value changes we classify more or less points as diabetic or non-diabetic. We notice that the decision boundary that was calculated will fit more with the higher r value (r=0.8). Higher r value will also predict more false negatives and fewer false positives.
Training Misclassification Error(r=0.2): 0.369403 
Training Misclassification Error(r=0.8): 0.3078358

5. What can you say about the quality of this model compared to the previous logistic regression model? 
How have the basis expansion trick affected the shape of the decision boundary and the prediction accuracy?
Training Misclassification Error, Base Expansion: 0.2406716 
Training Misclassification Error, Original: 0.261194
The decision boundary for the basis expanded model seems non-linear, as the additional features allow the model to capture more complex relationships between 
$x_1$(Plasma glucose) and $x_2$(Age). We can see that the misclassification error for basis expansion is lower compared to the misclassification of the original model, since the non-linear boundary better captures the structure of the data. We should also mention that there is now a slight risk of overfitting if the dataset is noisy





```{r Task 3}
#######################Q1#######################

mydata <- read.csv("C:/Users/Ana/Documents/LiU University/Machine Learning/csv data for labs/Lab 1 data/pima-indians-diabetes.csv")

#Creating scatterplot
plot(x = mydata[, 8], 
     y = mydata[, 2],
     xlab = "Age",
     ylab = "Plasma glucose concentration",
     main = "Age vs Plasma glucose concentration",
     col = as.factor(mydata[, 9]))

#######################Q2#######################

#Separating into train/test data

X = mydata[, c(2, 8)]  
y = mydata[, 9]        


n = dim(mydata)[1]    
set.seed(12345)        
id = sample(1:n, floor(n*0.7))  

X_train = X[id, ] 
colnames(X_train) = c("Glucose", "Age")
y_train = y[id]        
X_test = X[-id, ]  
X_test = as.data.frame(X_test)
colnames(X_test) = c("Glucose", "Age")
y_test = y[-id]        

train_data = data.frame(Glucose = X_train[, 1], Age = X_train[, 2], Diabetes = y_train)

#Training logistic regression model
logit_model = glm(y_train ~ Glucose + Age,data = train_data, family = binomial)
#x_1 is Plasma glucose
#x_2 is Age
summary(logit_model)

# Extract coefficients
intercept = coef(logit_model)[1]
coef_x1 = coef(logit_model)[2]  
coef_x2 = coef(logit_model)[3]  

# Probabilistic equation
cat("P(y = 1 | x1, x2) = 1 / (1 + exp(-(", intercept, "+", coef_x1, "* x1 +", coef_x2, "* x2)))\n")

# Predict probabilities and classify for training set
train_probs = predict(logit_model, newdata = data.frame(Glucose = X_train[, 1], Age = X_train[, 2]), type = "response")
train_preds = ifelse(train_probs >= 0.5, 1, 0)
train_error = mean(train_preds != y_train)
cat("Training Misclassification Error:", train_error, "\n")


# Predict probabilities and classify for test set
test_probs = predict(logit_model, newdata = X_test, type = "response")
test_preds = ifelse(test_probs >= 0.5, 1, 0)

#Creating new, predict scatterplot
plot(x = X_test[,2], 
     y = X_test[,1],
     xlab = "Age",
     ylab = "Plasma glucose concentration",
     main = "Predicted Diabetes",
     col = as.factor(test_preds))

#######################Q3#######################
decision_boundary <- function(x_1) {
  -(intercept/coef_x1) - (coef_x2/coef_x1) * x_1}

curve(decision_boundary(x), add = TRUE, col = "blue", lwd = 2)

#######################Q4#######################

#  r = 0.2

# Predict probabilities and classify for training set
train_probs = predict(logit_model, newdata = data.frame(Glucose = X_train[, 1], Age = X_train[, 2]), type = "response")
train_preds = ifelse(train_probs >= 0.2, 1, 0)
train_error = mean(train_preds != y_train)
cat("Training Misclassification Error:", train_error, "\n")


# Predict probabilities and classify for test set
test_probs = predict(logit_model, newdata = X_test, type = "response")
test_preds = ifelse(test_probs >= 0.2, 1, 0)

#Creating new, predict scatterplot
plot(x = X_test[,2], 
     y = X_test[,1],
     xlab = "Age",
     ylab = "Plasma glucose concentration",
     main = "Predicted Diabetes",
     col = as.factor(test_preds))



#  r = 0.8

# Predict probabilities and classify for training set
train_probs = predict(logit_model, newdata = data.frame(Glucose = X_train[, 1], Age = X_train[, 2]), type = "response")
train_preds = ifelse(train_probs >= 0.8, 1, 0)
train_error = mean(train_preds != y_train)
cat("Training Misclassification Error:", train_error, "\n")


# Predict probabilities and classify for test set
test_probs = predict(logit_model, newdata = X_test, type = "response")
test_preds = ifelse(test_probs >= 0.8, 1, 0)

#Creating new, predict scatterplot
plot(x = X_test[,2], 
     y = X_test[,1],
     xlab = "Age",
     ylab = "Plasma glucose concentration",
     main = "Predicted Diabetes",
     col = as.factor(test_preds))

#######################Q5#######################

# Defining functions for new features
z_1 <- function(x_1){
  x_1^4
}

z_2 <- function(x_1,x_2){
  (x_1^3)*(x_2)
}

z_3 <- function(x_1,x_2){
  (x_1^2)*(x_2^2)
}

z_4 <- function(x_1,x_2){
  (x_1)*(x_2^3)
}

z_5 <- function(x_2){
  x_2^4
}

# Computing new features for the train dataset
train_data_base_exp <- data.frame(
  Glucose = X_train[, 1],
  Age = X_train[, 2],
  z1 = z_1(X_train[, 1]),
  z2 = z_2(X_train[, 1], X_train[, 2]),
  z3 = z_3(X_train[, 1], X_train[, 2]),
  z4 = z_4(X_train[, 1], X_train[, 2]),
  z5 = z_5(X_train[, 2]),
  Diabetes = y_train
)

# Model with more features
logit_model_base_exp = glm(Diabetes ~ Glucose + Age + z1 + z2 + z3 + z4 + z5, data = train_data_base_exp, family = binomial)
summary(logit_model_base_exp)

# Predicting probabilities and classifying for the training set
train_probs_base_exp <- predict(logit_model_base_exp, newdata = train_data_base_exp, type = "response")
train_preds_base_exp <- ifelse(train_probs_base_exp >= 0.5, 1, 0)
train_error_base_exp = mean(train_preds_base_exp != y_train)
cat("Training Misclassification Error, Base Expansion:", train_error_base_exp, "\n")

# Computing new features for the test dataset
test_data_base_exp <- data.frame(
  Glucose = X_test[, 1],
  Age = X_test[, 2],
  z1 = z_1(X_test[, 1]),
  z2 = z_2(X_test[, 1], X_test[, 2]),
  z3 = z_3(X_test[, 1], X_test[, 2]),
  z4 = z_4(X_test[, 1], X_test[, 2]),
  z5 = z_5(X_test[, 2])
)

# Predicting probabilities and classifying for the test set
test_probs_base_exp <- predict(logit_model_base_exp, newdata = test_data_base_exp, type = "response")
test_preds_base_exp <- ifelse(test_probs_base_exp >= 0.5, 1, 0)

#Creating new, predict scatterplot
plot(x = X_test[,2], 
     y = X_test[,1],
     xlab = "Age",
     ylab = "Plasma glucose concentration",
     main = "Predicted Diabetes (Basis Expansion)",
     col = as.factor(test_preds_base_exp))

```

