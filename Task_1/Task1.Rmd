---
title: "Task 1: Lab 1: Block 1"
output: pdf_document
---

```{r setup , echo=FALSE}
library(kknn)
library(tidyverse)
setwd("C:/Users/Sofia Danielson/OneDrive - Linköpings universitet/R programmering/maskininlärning/ML_lab1")
#read in data
data <- read_csv("Task_1/optdigits.csv")


colnames(data)[ncol(data)] <- "digit"
data$digit <- factor(data$digit)
#partition data 
n=dim(data)[1]
set.seed(12345) 
id=sample(1:n, floor(n*0.5)) 
train=data[id,] 
id1=setdiff(1:n, id)
set.seed(12345) 
id2=sample(id1, floor(n*0.25)) 
valid=data[id2,]
id3=setdiff(id1,id2)
test=data[id3,]
```

2. Use training data to fit 30-nearest neighbor classifier with function kknn() and
kernel=”rectangular” from package kknn and estimate
• Confusion matrices for the training and test data (use table())
• Misclassification errors for the training and test data
Comment on the quality of predictions for different digits and on the overall
prediction quality:


```{r classification errors , echo=FALSE}
# Predict on training data
knn_model <- kknn(digit ~ ., train = train, test = train, k = 30, kernel = "rectangular")
train_pred <- fitted(knn_model)

# Predict on test data
test_knn_model <- kknn(digit ~ ., train = train, test = test, k = 30, kernel = "rectangular")
test_pred <- fitted(test_knn_model)

# Misclassification error
train_misclassification <- sum(train$digit != train_pred) / nrow(train)
print(paste("Training Misclassification Error:", round(train_misclassification, 4)))

test_misclassification <- sum(test$digit != test_pred) / nrow(test)
print(paste("Test Misclassification Error:", round(test_misclassification, 4)))

```

The misclassification level for both the train and test data seem to be at a fairly low level, at 0.0424 and 0.0492 respectively

Based from the calculations of the error rate for each digit on the training and testing data, the digits that appears the easiest to classify correctly for the knn model are 0, 2, 3, 6 and 7

The digits that seem to be the harder to predict are 1, 4, 5, 8, 9

```{r digit errors , echo=FALSE}
#confusion matrices
train_conf_matrix <- table(train$digit, train_pred)
print("Training Confusion Matrix:")
print(train_conf_matrix)

test_conf_matrix <- table(test$digit, test_pred)
print("Test Confusion Matrix:")
print(test_conf_matrix)

# Misclassification for each digit:
train_correctly_classified <- diag(as.matrix(train_conf_matrix))
test_correctly_classified <- diag(as.matrix(test_conf_matrix))

error_rate_digit_train <- 1 - (train_correctly_classified/rowSums(as.matrix(train_conf_matrix)))
error_rate_digit_test <- 1 - (test_correctly_classified/rowSums(as.matrix(test_conf_matrix)))

print(error_rate_digit_train)
print(error_rate_digit_test)
```

3. Find any 2 cases of digit “8” in the training data which were easiest to classify
and 3 cases that were hardest to classify (i.e. having highest and lowest
probabilities of the correct class). Comment on whether
these cases seem to be hard or easy to recognize visually.


The first two digits seemed pretty definitely easier to recognize as an eight (altought the first one of them was maybe a little difficult to recognize as well). The other did appear much less recognizable as symbolizing the digit eight. In fact, looking at the probabilities for the predictions for them, you get the result down in the table below, indicating that althought the probability for the datapoints being 8 were not zero, the model calculated it much more likely for the datapoints to resemble other digits: 


```{r digit cases , echo=FALSE}
# # Extract predicted probabilities and identify cases for digit "8"
train_probs <- knn_model$prob
# 
# Filter for only rows where the true digit is "8"
eight_indices <- which(train$digit == 8)
# 
# # Get probabilities of the correct class ("8") for these cases as a numeric vector
eight_probs <- vapply(eight_indices, function(i) as.numeric(train_probs[i, "8"]), numeric(1))

# Find indices of the 2 highest and 3 lowest probabilities
easiest_cases <- order(eight_probs, decreasing = TRUE)[1:2]
hardest_cases <- order(eight_probs)[1:3]

easiest_indexes <- eight_indices[easiest_cases]
hardest_indexes <- eight_indices[hardest_cases]

# Function to visualize a digit as an 8x8 heatmap
visualize_digit <- function(data_row) {
  matrix_data <- matrix(as.numeric(data_row[1:64]), nrow = 8, ncol = 8, byrow = TRUE)
  heatmap(matrix_data, Colv = NA, Rowv = NA, scale = "none",
           col = heat.colors(16))
}

# Visualize easiest cases
cat("Easiest cases of digit '8':\n")
for (i in easiest_indexes) {
  visualize_digit(train[i,])
}

# Visualize hardest cases
cat("Hardest cases of digit '8':\n")
for (i in hardest_indexes) {
  visualize_digit(train[i,])
}

#predictions for hardest cases of 8
cat("probabilities for hardest cases of digit '8':\n")
print(knn_model$prob[hardest_indexes,])

cat("probabilities for easiest cases of digit '8':\n")
print(knn_model$prob[easiest_indexes,])

```


4. How does the model complexity
change when K increases and how does it affect the training and validation? estimate the test error for the model having the optimal K, compare it with the training and validation
errors and make necessary conclusions about the model quality:

As observed from the graph below, the misclassification seems to be generally lower for predictions on the training data than the validation data, which is to be expected since the model is trained to minimize the error on the training data. The error appears to only increase with K for the training data. This is simply because the model becomes more sensitive to local anomalies and is more likely to overfit to the training data for few K's. The validation error decreases a little bit from 1 to 7 K, but later grows as K also grows. The model was probably slightly overfit for the lower values of K, but reached a local optimum at around K = 7. As K grew, it probably began to become underfitted as it averages to many neighbors, including those far away from the test point, which explains why the validation error began to grow after K > 7.


```{r error depending ok K , echo=FALSE}
train_errors <- numeric(30)  
valid_errors <- numeric(30)  

for (k in 1:30) {
  # Fit k-NN model
  knn_model <- kknn(digit ~ ., train = train, test = valid, k = k, kernel = "rectangular")
  
  # Training predictions
  train_pred <- fitted(kknn(digit ~ ., train = train, test = train, k = k, kernel = "rectangular"))
  train_errors[k] <- mean(train_pred != train$digit)
  
  # Validation predictions 
  valid_pred <- fitted(knn_model)
  valid_errors[k] <- mean(valid_pred != valid$digit)
}

par(xpd = TRUE, mar = c(5, 4, 4, 7))  # Increase the right margin
plot(1:30, train_errors, type = "o", col = "blue", ylim = c(0, max(train_errors, valid_errors)), 
     xlab = "K", ylab = "Misclassification Error", main = "Training and Validation Errors vs K")
lines(1:30, valid_errors, type = "o", col = "red")
legend("bottomright", inset = c(-0.2, 0), legend = c("Training Error", "Validation Error"), 
       col = c("blue", "red"), lty = 1, pch = 1, cex = 0.8)

# Find optimal K (minimizing validation error)
optimal_k <- which.min(valid_errors)
cat("Optimal K:", optimal_k, "\n")

# Estimate the test error for the optimal K
knn_test <- kknn(digit ~ ., train = train, test = test, k = optimal_k, kernel = "rectangular")
test_pred <- fitted(knn_test)
test_error <- mean(test_pred != test$digit)
cat("Optimal K:", optimal_k, "\n")
cat("Test Error for optimal K:", test_error, "\n")
```








